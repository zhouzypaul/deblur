import os

import cv2
import numpy as np
import matplotlib.pyplot as plt
from scipy.fft import fft2, ifft2
from scipy.signal import convolve2d, convolve
from scipy.optimize import minimize
from scipy.stats import mode

from conjugate_gradient import conjugate_gradient
from get_data import parse_dataset
from pypher import otf2psf, psf2otf
import params as hp


"""
Deblurs images based on iterative latent image and blur kernel estimation
"""


def deblur(blur_img):
    """
    deblurs an image by estimating the blur kernel using
    a coarse-to-fine image pyramid approach

    args:
        blur_img: the blur image
    return:
        final blur kernel and list of estimated latent images
    """
    hp.lmda = 4e-3

    # image pyramid generated by downsampling blur_img
    image_pyramid = generate_image_pyramid(blur_img)

    # previous blur_kernel
    k = init_kernel()

    # intermediate latent images
    latent_imgs = []

    # coarse to fine
    for y in reversed(image_pyramid):

        # perform Algorithm 2 to estimate intermediate estimated latent and blur kernel
        k, x = estimate_blur_kernel(y, k)

        latent_imgs.append(x)

    # perform final deconvolve using final estimated kernel
    k, final_deblurred = estimate_blur_kernel(y, k)
    latent_imgs.append(final_deblurred)

    k = cv2.rotate(k, cv2.ROTATE_180)

    return threshold_text(final_deblurred), k


def estimate_blur_kernel(blur_img, kernel):
    """
    estimates the blur kernel using an interative process as
    outlined in Algorithm 2 of the paper

    args:
        blur_img: the blur image
        kernel: the blur kernel estimate
    return:
        blur kernel and intermediate latent image
    """
    # intermediate latent image
    x = None
    k = kernel.copy()

    # preset number of iterations, paper uses 5
    for _ in range(3):

        # solve for x using Algorithm 1
        x = solve_latent(blur_img, k)

        # solve for k, update kernel estimate eq. (12)
        k = solve_kernel(x, blur_img, hp.lmda)

        # update lambda hyperparameter
        hp.lmda = max(hp.lmda / 1.1, 1e-4)

    return k, x


def solve_latent(blur_img, kernel):
    """
    solves for the latent image according to algorithm 1

    args:
        blur_img: the blur image
        kernel: the blur kernel estimate
    return:
        intermediate latent image
    """
    latent = blur_img
    out_shape = blur_img.shape

    # precompute
    fx = psf2otf(np.array([[1, -1]]), out_shape)
    fy = psf2otf(np.array([[1, -1]]).T, out_shape)
    ker = psf2otf(kernel, out_shape)
    ker_ker = np.abs(ker) ** 2
    nablas = np.abs(fx) ** 2 + np.abs(fy) ** 2
    ker_latent = np.conj(ker) * fft2(latent)

    beta = 2 * hp.lmda * hp.sigma
    while beta < hp.beta_max:
        u = solve_u(latent, beta)
        miu = 2 * hp.lmda

        while miu < hp.miu_max:
            fg = compute_fg(latent, miu)
            nom = ker_latent + beta * fft2(u) + miu * fft2(fg)
            denom = ker_ker + beta + miu * nablas
            latent = np.real(ifft2(nom / denom))
            miu *= 2

        beta *= 2

    return latent


def solve_kernel(latent_img, blur_img, weight):
    """
    this function estimates the blur kernel using an interative process as
    outlined in Algorithm 2 of the paper

    args:
        y: the blur image
        k: the blur kernel estimate
    return:
        blur kernel and intermediate latent image
    """

    # derivative kernels
    dx = np.array([[-1, 1], [0, 0]])
    dy = np.array([[-1, 0], [1, 0]])

    lxf = fft2(convolve2d(latent_img, dx, mode='valid'))
    lyf = fft2(convolve2d(latent_img, dy, mode='valid'))

    bxf = fft2(convolve2d(blur_img, dx, mode='valid'))
    byf = fft2(convolve2d(blur_img, dy, mode='valid'))

    b_f = np.conj(lxf) * bxf + np.conj(lyf) * byf
    b = np.real(otf2psf(b_f, hp.kernel_size))

    p = {}
    p['m'] = np.conj(lxf) * lxf + np.conj(lyf) * lyf
    p['img_size'] = np.shape(bxf)
    p['kernel_size'] = hp.kernel_size
    p['lmda'] = weight

    psf = np.ones(hp.kernel_size) / np.prod(hp.kernel_size)
    psf = conjugate_gradient(psf, b, 20, 1e-5, compute_ax, p)

    psf = np.where(psf >= 0.05 * np.max(psf), psf, 0)
    psf /= np.sum(psf)
    return psf


# ==============================================================================


# HELPERS ======================================================================


def init_kernel():
    '''
    Initializes kernel estimate

    args:
        kernel_size: size of kernel to estimate
    returns:
        kernel: 2d np array
    '''
    n, _ = hp.kernel_size
    kernel = np.zeros(hp.kernel_size)
    mid = (n - 1) // 2

    # set middle two pixels to 1/2
    kernel[mid, mid:mid + 2] = 1 / 2
    return kernel


def generate_image_pyramid(y):
    '''
    Repeatedly downsamples blurred image with bilinear interpolation

    args:
        y: single image - numpy array
    returns:
        list of images - list of numpy arrays
    '''
    img = y.copy()
    try:
        assert isinstance(img, np.ndarray)
    except AssertionError:
        img = np.array(img)
    image_pyramid = [img]

    # downsample for fixed number of layers
    for _ in range(5):
        layer = cv2.pyrDown(np.array(image_pyramid[-1]))
        layer = cv2.pyrUp(layer)
        image_pyramid.append(layer)

    return image_pyramid


def solve_u(latent, beta):
    '''
    Solve for u in Algorithm 1

    args:
        latent: the latent image
        beta: hyperparameter
    returns:
        u as described in the paper
    '''
    threshold = hp.lmda * hp.sigma / beta
    return np.where(latent ** 2 >= threshold, latent, 0)


def solve_g(h, v, miu):
    '''
    Solve for g in Algorithm 1

    args:
        h: horizontal component
        v: vertical component
        miu: hyperparameter
    returns:
        g as described in the paper
    '''
    condition = h ** 2 + v ** 2 >= hp.lmda / miu
    return np.where(condition, h, 0), np.where(condition, v, 0)


def compute_fg(latent, miu):
    '''
    Compute F_G for Algorithm 1
    '''
    # compute horizontal and vertical gradients to solve for g
    h_diff = latent[:, 0] - latent[:, -1]
    h = np.hstack((np.diff(latent, axis=1), h_diff[:, None]))
    v_diff = latent[0, :] - latent[-1, :]
    v = np.vstack((np.diff(latent, axis=0), v_diff))
    gh, gv = solve_g(h, v, miu)

    # compute horizontal and vertical components in FG
    gh_diff = gh[:, -1] - gh[:, 0]
    h_component = np.hstack((gh_diff[:, None], -np.diff(gh, axis=1)))
    gv_diff = gv[-1, :] - gv[0, :]
    v_component = np.vstack((gv_diff, -np.diff(gv, axis=0)))

    return h_component + v_component


def compute_ax(x, p):
    '''
    Computes Ap term for system of linear equations

    args:
        x: x from Ax=b
        p: dict of function parameterss
    returns:
        Axpterm
    '''
    xf = psf2otf(x, p['img_size'])
    ap = otf2psf(p['m'] * xf, p['kernel_size'])
    ap += p['lmda'] * x
    return ap


def visualize_results(blurred, deblurred, kernel, est_kernel):
    fig = plt.figure()
    fig.add_subplot(2, 2, 1)
    plt.title('Blurred')
    plt.imshow(blurred, cmap='gray')
    fig.add_subplot(2, 2, 2)
    plt.title('Deblurred')
    plt.imshow(deblurred, cmap='gray')
    fig.add_subplot(2, 2, 3)
    plt.title('Kernel')
    plt.imshow(kernel, cmap='gray')
    fig.add_subplot(2, 2, 4)
    plt.title('Estimated Kernel')
    plt.imshow(est_kernel, cmap='gray')
    plt.tight_layout()
    plt.show()
    plt.close()


# ==============================================================================
# remove artifacts
def min_w(w, beta, v, a):
    return np.power(np.abs(w), a) + (beta / 2.0) * np.square(w - v)


def snr(im1, im2):
    im1 = im1.reshape((-1, im1.shape[2]))
    im2 = im2.reshape((-1, im2.shape[2]))
    mean = np.sum(im1, axis=0) / len(im1)
    nom = np.sum(np.square(im1 - mean), axis=0)
    denom = np.sum(np.square(im1 - im2), axis=0)
    avgs = nom / denom
    return np.sum(avgs) / im1.shape[1]

    
def get_LUT(a):
    if os.path.exists(f"LUT-{a}.npy"):
        table = np.load(f"LUT-{a}.npy")
    else:
        nv = 10000
        nbeta = 16
        vmin, vmax = -0.6, 0.6
        vs = np.linspace(vmin, vmax, num=nv)
        betas = np.power(np.sqrt(2), np.arange(nbeta))
        table = np.zeros((nv, nbeta))

        for i in range(nv):
            print(i)
            if i % 1000 == 0:
                print(i)
            for j in range(nbeta):
                sol = minimize(min_w, 0, args=(betas[j], vs[i], a))
                table[i, j] = sol.x[0]

        np.save(f"LUT-{a}", table)
    return table


def solve_w(x, beta, f1, f2, LUT):
    v1 = convolve(x, np.repeat(f1[:,:,None], x.shape[2], axis=2), mode="same")
    v2 = convolve(x, np.repeat(f2[:,:,None], x.shape[2], axis=2), mode="same")
    v1, v2 = np.clip(v1, -0.6, 0.6), np.clip(v2, -0.6, 0.6)

    r1 = np.clip(np.searchsorted(np.linspace(-0.6, 0.6, num=10000), v1), 0, 9999)
    c1 = np.searchsorted(np.power(np.sqrt(2), np.arange(16)), beta) * np.ones(v1.shape)
    w1 = LUT[r1, c1.astype(np.int32)]

    r2 = np.clip(np.searchsorted(np.linspace(-0.6, 0.6, num=10000), v2), 0, 9999)
    c2 = np.searchsorted(np.power(np.sqrt(2), np.arange(16)), beta) * np.ones(v2.shape)
    w2 = LUT[r2, c2.astype(np.int32)]

    return w1, w2


def remove_artifact(blur_img, k, l):
    """
    this removes the artifacts as detailed in section 3.3 of the paper
    """
    y = blur_img/255.0
    a = 0.5
    b0 = 1
    binc = np.sqrt(2)
    bmax = 256
    LUT = get_LUT(a)
    beta = b0
    x = y
    k = k / np.sum(k)
    k = k[::-1, ::-1]

    f1, f2 = np.array([[1,-1]]), np.array([[1],[-1]])
    fftsize = (x.shape[0] + k.shape[0] + f1.shape[0] + f2.shape[0], x.shape[1] + k.shape[1] + f1.shape[1] + f2.shape[1])
    fftsize = np.power(2, np.ceil(np.log2(fftsize))).astype(int)
    f1fft = np.fft.fft2(f1, axes=(0, 1), s=fftsize)
    f2fft = np.fft.fft2(f2, axes=(0, 1), s=fftsize)
    kfft = np.fft.fft2(k, axes=(0, 1), s=fftsize)
    yfft = np.fft.fft2(y, axes=(0, 1), s=fftsize)
    
    f1fft = np.repeat(f1fft[:,:,None], x.shape[2], axis=2)
    f2fft = np.repeat(f2fft[:,:,None], x.shape[2], axis=2)
    kfft = np.repeat(kfft[:,:,None], x.shape[2], axis=2)

    while beta < bmax:
        w1, w2 = solve_w(x, beta, f1, f2, LUT)
        w1fft = np.fft.fft2(w1, axes=(0, 1), s=fftsize)
        w2fft = np.fft.fft2(w2, axes=(0, 1), s=fftsize)

        nom = np.conjugate(f1fft) * w1fft + np.conjugate(f2fft) * w2fft + (l / beta) * np.conjugate(kfft)*yfft
        denom = np.conjugate(f1fft) * f1fft + np.conjugate(f2fft) * f2fft + (l / beta) * np.conjugate(kfft) * kfft
        x = np.fft.ifft2(nom / denom, axes=(0, 1))[0:y.shape[0],0:y.shape[1]].real
        x = np.clip(x, 0, 1)
        beta = binc * beta
    return x


def threshold_text(x):
    img = 255 * (x - np.min(x)) / (np.max(x) - np.min(x))
    img = np.uint8(img)
    bg = mode(img, axis=None)[0][0] * 0.9
    (_, thresh) = cv2.threshold(img, bg, 255, cv2.THRESH_BINARY)
    return thresh


def main():
    """
    interleave kernel and latent image estimation
    load the original images and deblur them, and plot them together
    """
    image_path = 'data/ieee2016/text-images/gt_images'
    kernel_path = 'data/ieee2016/text-images/kernels'
    ground_truth_images, blurred_images = parse_dataset(
        image_path, kernel_path)  # there should be 15 of them

    for _, blurs in zip(ground_truth_images, blurred_images):
        rind = np.random.randint(len(blurs))
        blur_img, kernel = blurs[rind]
        latent, est_kernel = deblur(blur_img)
        visualize_results(blur_img, latent, kernel, est_kernel)


if __name__ == '__main__':
    np.random.seed(0)
    main()
